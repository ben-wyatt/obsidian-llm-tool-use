{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a865a950",
   "metadata": {},
   "source": [
    "Links\n",
    "- [DSPy Documentation](https://dspy.ai/#__tabbed_1_4)\n",
    "\n",
    "\n",
    "Double Saved to `obsidian-llm-tool-use` Github repo and my personal note vault in obsidian via:\n",
    "```bash\n",
    "jupytext --to markdown dspy_modules/intro_to_dspy.ipynb -o ~/Obsidian/Notes\\ Vault/intro_to_dspy.md\n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"DSPy-Components.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09674048",
   "metadata": {},
   "source": [
    "First, import the package and setup your llm calling configuration. For this, we'll be using ollama.\n",
    "\n",
    "Make sure to spin up your ollama server using\n",
    "```bash\n",
    "ollama start\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de02a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.21\n"
     ]
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "print(importlib.metadata.version('dspy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5efa3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ben.Wyatt/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "lm = dspy.LM('openai/qwen2.5:7b-instruct-q4_K_M', \n",
    "             api_base='http://localhost:11434/v1', \n",
    "             api_key='', \n",
    "             cache=False)\n",
    "\n",
    "dspy.configure(lm=lm)\n",
    "dspy.enable_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e980b6f0",
   "metadata": {},
   "source": [
    "Lets do the basic prompt-response: just use the `lm` as a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8b1023",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "litellm.NotFoundError: NotFoundError: OpenAIException - Resource not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:724\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[39m\n\u001b[32m    723\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:652\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[39m\n\u001b[32m    638\u001b[39m logging_obj.pre_call(\n\u001b[32m    639\u001b[39m     \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m    640\u001b[39m     api_key=openai_client.api_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m     },\n\u001b[32m    647\u001b[39m )\n\u001b[32m    649\u001b[39m (\n\u001b[32m    650\u001b[39m     headers,\n\u001b[32m    651\u001b[39m     response,\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_sync_openai_chat_completion_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m logging_obj.model_call_details[\u001b[33m\"\u001b[39m\u001b[33mresponse_headers\u001b[39m\u001b[33m\"\u001b[39m] = headers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:149\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:471\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_client, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:453\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_client, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m     raw_response = \u001b[43mopenai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    924\u001b[39m validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1236\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/openai/_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1033\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/litellm/main.py:1799\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   1793\u001b[39m     logging.post_call(\n\u001b[32m   1794\u001b[39m         \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m   1795\u001b[39m         api_key=api_key,\n\u001b[32m   1796\u001b[39m         original_response=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m   1797\u001b[39m         additional_args={\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: headers},\n\u001b[32m   1798\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1799\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optional_params.get(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1802\u001b[39m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/litellm/main.py:1772\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1772\u001b[39m     response = \u001b[43mopenai_chat_completions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1773\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1774\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1775\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1776\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1779\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1780\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1781\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   1786\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[32m   1788\u001b[39m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m=\u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1790\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1792\u001b[39m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:735\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[39m\n\u001b[32m    734\u001b[39m     error_headers = \u001b[38;5;28mgetattr\u001b[39m(error_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    736\u001b[39m     status_code=status_code,\n\u001b[32m    737\u001b[39m     message=error_text,\n\u001b[32m    738\u001b[39m     headers=error_headers,\n\u001b[32m    739\u001b[39m     body=error_body,\n\u001b[32m    740\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mazure_lm\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWho are you?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# => ['This is a test!']\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/dspy/utils/callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m callbacks = _get_active_callbacks(instance)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n\u001b[32m    330\u001b[39m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/dspy/clients/base_lm.py:89\u001b[39m, in \u001b[36mBaseLM.__call__\u001b[39m\u001b[34m(self, prompt, messages, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt=\u001b[38;5;28;01mNone\u001b[39;00m, messages=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._process_lm_response(response, prompt, messages, **kwargs)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/dspy/clients/lm.py:107\u001b[39m, in \u001b[36mLM.forward\u001b[39m\u001b[34m(self, prompt, messages, **kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cache_in_memory:\n\u001b[32m    105\u001b[39m     completion = cached_litellm_completion \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_type == \u001b[33m\"\u001b[39m\u001b[33mchat\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m cached_litellm_text_completion\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     results = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    112\u001b[39m     completion = litellm_completion \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_type == \u001b[33m\"\u001b[39m\u001b[33mchat\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m litellm_text_completion\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/dspy/clients/cache.py:224\u001b[39m, in \u001b[36mrequest_cache.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_result\n\u001b[32m    223\u001b[39m \u001b[38;5;66;03m# Otherwise, compute and store the result\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m cache.put(modified_request, result)\n\u001b[32m    227\u001b[39m cache.ignored_args_for_cache_key = original_ignored_args_for_cache_key\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/dspy/clients/lm.py:218\u001b[39m, in \u001b[36mcached_litellm_completion\u001b[39m\u001b[34m(request, num_retries)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     litellm_cache_args = {\u001b[33m\"\u001b[39m\u001b[33mno-cache\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mno-store\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_cache_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/dspy/clients/lm.py:240\u001b[39m, in \u001b[36mlitellm_completion\u001b[39m\u001b[34m(request, num_retries, cache)\u001b[39m\n\u001b[32m    236\u001b[39m caller_predict = dspy.settings.caller_predict\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    238\u001b[39m     \u001b[38;5;66;03m# If `streamify` is not used, or if the exact predict doesn't need to be streamed,\u001b[39;00m\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# we can just return the completion without streaming.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mretry_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# The stream is already opened, and will be closed by the caller.\u001b[39;00m\n\u001b[32m    247\u001b[39m stream = cast(MemoryObjectSendStream, stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/litellm/utils.py:1255\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1252\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1253\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1254\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1255\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/litellm/utils.py:1133\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1131\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1132\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1134\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/litellm/main.py:3184\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3181\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3183\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3187\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3190\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2217\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2216\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2217\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2218\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2219\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/personal/obsidian-llm-tool-use/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:405\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m404\u001b[39m:\n\u001b[32m    404\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFoundError(\n\u001b[32m    406\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNotFoundError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    407\u001b[39m         model=model,\n\u001b[32m    408\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    409\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    410\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m    411\u001b[39m     )\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m408\u001b[39m:\n\u001b[32m    413\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mNotFoundError\u001b[39m: litellm.NotFoundError: NotFoundError: OpenAIException - Resource not found"
     ]
    }
   ],
   "source": [
    "lm(\"Who are you?\", temperature=0.7)  # => ['This is a test!']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78536af",
   "metadata": {},
   "source": [
    "Super compact syntax.\n",
    "\n",
    "You could just use this as a nice way to make your LLM calls a bit more pythonic.\n",
    "\n",
    "You can also send using the chat completions formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf846a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm(messages=[{\"role\": \"user\", \"content\": \"Say this is a test!\"}])  # => ['This is a test!']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555bb837",
   "metadata": {},
   "source": [
    "Ok now to the first main topic:\n",
    "\n",
    "## Modules\n",
    "\n",
    "Modules help you describe AI behavior as *code*. not *strings*.\n",
    "\n",
    "You specify a *Signature*: a string that defines an action via inputs and outputs behavior: `\"question -> answer: float\"`\n",
    "\n",
    "Then you select a *Module* to assign a strategy for invoking the LLM. `Predict` is the simplest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa9660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_math = dspy.Predict(\"question -> answer: float\")\n",
    "result = solve_math(question=\"What is 1 + 1?\")\n",
    "print(result.completions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e99f1",
   "metadata": {},
   "source": [
    "A Module:\n",
    "- wraps a signature.\n",
    "- is callable\n",
    "- carries \"learnable parameters\" that DSPy can run optimization on.\n",
    "- composes: modules call other modules, can be stored as `json`, or be nested inside larger `dspy.Program` graphs.\n",
    "- persists: `module.save()`/`load()` for controlling state.\n",
    "\n",
    "\n",
    "There's a few really powerful primative ones already implemented, like `dspy.ChainOfThought`. It automatically:\n",
    "1. Inserts an instruction telling the LLM to show its reasoning.\n",
    "2. Adds an implicit extra output field called `reasoning`.\n",
    "3. Returns both the reasoning and the final answer, while still respecting the original signature.\n",
    "\n",
    "So that's why it won't be very good for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = solve_math(question=\"What is the third root of 963261?\")\n",
    "print(result.completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6218c0e8",
   "metadata": {},
   "source": [
    "But you can get pretty creative with the signatures.  LLM act as this universal function approximator written via English.  Modules try to shape that approximator into a math function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_riddle = dspy.Predict(\"riddle -> answer\")\n",
    "print(solve_riddle(riddle=\"What has keys but can't open locks?\").answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff51c38",
   "metadata": {},
   "source": [
    "In your *Signature* you can list multiple fields: `\"context: list[str], question -> answer\"` or omit the types if they're strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojify = dspy.Predict(\"story -> emoji_sequence\")\n",
    "\n",
    "story=\"\"\"You're walking in the woods\n",
    "There's no one around and your phone is dead\n",
    "Out of the corner of your eye you spot him\n",
    "Shia LaBeouf\n",
    "\n",
    "He's following you, about 30 feet back\n",
    "He gets down on all fours and breaks into a sprint\n",
    "He's gaining on you\n",
    "Shia LaBeouf\n",
    "\n",
    "You're looking for you car but you're all turned around\n",
    "He's almost upon you now\n",
    "And you can see there's blood on his face\n",
    "My God, there's blood everywhere!\n",
    "\n",
    "Running for you life (from Shia LaBeouf)\n",
    "He's brandishing a knife (it's Shia LaBeouf)\n",
    "Lurking in the shadows\n",
    "Hollywood superstar Shia LaBeouf\n",
    "\n",
    "Living in the woods (Shia LaBeouf)\n",
    "Killing for sport (Shia LaBeouf)\n",
    "Eating all the bodies\n",
    "Actual cannibal Shia LaBeouf\"\"\"\n",
    "\n",
    "emoji_sequence = emojify(story=story)\n",
    "print(emoji_sequence.emoji_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d27890",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate = dspy.Predict(\"string -> italian\")\n",
    "translation = translate(string=story)\n",
    "print(translation.italian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e506541",
   "metadata": {},
   "source": [
    "### Single-shot predictors\n",
    "\n",
    "There's `Predict`, `ChainOfThought`, and `ChainOfThoughtWithHint` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_time_complexity = dspy.ChainOfThought(\"function -> time_complexity\")\n",
    "\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "    \n",
    "complexity = find_time_complexity(function=fibonacci)\n",
    "print(complexity.completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_hint = dspy.ChainOfThoughtWithHint(\"question -> answer: float\")\n",
    "prediction = cot_hint(question=\"What is 16 x 17?\", hint=\"16x10=160 and 16x7=112\")  \n",
    "print(prediction.reasoning)\n",
    "print(\"----\")\n",
    "print(prediction.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060bec9d",
   "metadata": {},
   "source": [
    "### Multi-shot Predictors\n",
    "\n",
    "Some built-in Modules use multiple LLM calls and tools to iteratively improve responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcbfd6f",
   "metadata": {},
   "source": [
    "`ReAct`: implements a *ReAct* agent pattern: the LLM alternates between thinking and calling user-supplied tooks, and stops when it fills the Signature. Used for search-and-answer agents, code-execution helpers, custom tool use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726249ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct tool counts number of letter occurances in a string\n",
    "def count_letters(string: str) -> dict:\n",
    "    counts = {}\n",
    "    for letter in string:\n",
    "        if letter.isalpha():\n",
    "            counts[letter] = counts.get(letter, 0) + 1\n",
    "    return counts\n",
    "\n",
    "question_answerer = dspy.ReAct(\"question -> answer\",tools=[count_letters],max_iters=3)\n",
    "\n",
    "print(question_answerer(question=\"How many R's in the word strawberry?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b250161",
   "metadata": {},
   "source": [
    "`ProgramOfThought`: ask LLM to write a python program, executes it, then passes result back into the answer. \n",
    "\n",
    "It relies on `Deno`, a code runtime for lightweight scripts, which needs to be installed using:\n",
    "\n",
    "```bash\n",
    "brew install deno\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84376233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import PythonInterpreter\n",
    "\n",
    "#test that the Python interpreter works\n",
    "print(PythonInterpreter()(\"print('Hello World!')\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ff7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pot = dspy.ProgramOfThought(\"question -> answer: float\", max_iters=5)\n",
    "\n",
    "code_gen = pot.code_generate(question=\"what is 5234 times 5324?\")\n",
    "#adding `.code_generate` to the end makes that the code doesn't execute. it just generates and returns the code.\n",
    "print(code_gen,\"=\"*15)\n",
    "\n",
    "\n",
    "result = pot(question=\"what is 5234 times 5324?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = pot(question=\"what is 5234 times 5324?\")\n",
    "result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pot_math = dspy.ProgramOfThought(\"question -> answer: float\", max_iters=5).code_generate\n",
    "result = pot_math(question=\"What is the  third root of 963261?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_attempt = pot_math(question=\"what is the eigth root of 52876533252\")  # raw string\n",
    "print(code_attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e41b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.inspect_history(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d5da93",
   "metadata": {},
   "source": [
    "`MultiChainComparison`: Spins up `M` separate `ChainOfThought` traces, asks the LLM to vote-critique, and returns the best. Fastest way to logarithmically scale intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99e725",
   "metadata": {},
   "source": [
    "### Your Own Modules and Signatures\n",
    "\n",
    "Starting from the built-in ones, you can construct your own modules that include multiple LLM calls and complicated flows.\n",
    "\n",
    "Custom signatures work similarly. You can define multiple `InputField()` and `OutputField()`'s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69bed707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, we'll tell it our birthday\n",
    "#and it figures out our sign\n",
    "class ZodiacSignature(dspy.Signature):\n",
    "    birth_day = dspy.InputField()\n",
    "    sign= dspy.OutputField(desc=\"Aries, Taurus, , Pisces\")\n",
    "\n",
    "\n",
    "#then it creates a horoscope.\n",
    "class HoroscopeSignature(dspy.Signature):\n",
    "    sign= dspy.InputField(desc='Zodiac sign')\n",
    "    current_date= dspy.InputField(desc='In YYYY-MM-DD format')\n",
    "    horoscope= dspy.OutputField(desc=\"One-paragraph horoscope\")\n",
    "    lucky_numbers= dspy.OutputField(desc=\"Comma-separated lucky numbers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0ca3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: My roommate Gaby was born January 29th 2001.\n",
      "Prediction(\n",
      "    sign='Aquarius',\n",
      "    horoscope_reasoning=\"Today's horoscope for Aquarius is influenced by the position of celestial bodies, which suggests a period of creativity and social engagement. The alignment of planets indicates an opportunity to connect with like-minded individuals or start new projects that align with your interests.\",\n",
      "    horoscope='As an Aquarius on June 2, 2025, you are in a phase where innovation and collaboration are key. Expect ideas to flow freely, making this a great day for brainstorming sessions or starting new ventures. Social interactions will be particularly rewarding as you connect with people who share your vision. However, be mindful of overextending yourself; balance is crucial.',\n",
      "    lucky_numbers='8, 23, 37, 41, 61'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "\n",
    "class HoroscopeFromBirthday(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predict_sign = dspy.Predict(ZodiacSignature)\n",
    "        self.create_horoscope=dspy.ChainOfThought(HoroscopeSignature)\n",
    "\n",
    "    def forward(self, birth_day:str):\n",
    "        #get the zodiac sign\n",
    "        sign_pred = self.predict_sign(birth_day=birth_day)\n",
    "        sign = sign_pred.sign\n",
    "        #create the horoscope\n",
    "        today = date.today()\n",
    "        horo_pred = self.create_horoscope(sign=sign,current_date=today)\n",
    "\n",
    "\n",
    "\n",
    "        #Prediction acts as the return type. A dictionary wrapper\n",
    "        return dspy.Prediction(\n",
    "            sign = sign,\n",
    "            horoscope_reasoning = horo_pred.reasoning,\n",
    "            horoscope = horo_pred.horoscope,\n",
    "            lucky_numbers = horo_pred.lucky_numbers\n",
    "        )\n",
    "\n",
    "\n",
    "zodiac_finder=HoroscopeFromBirthday()\n",
    "text = \"My roommate Gaby was born January 29th 2001.\"\n",
    "print('Input:', text)\n",
    "print(zodiac_finder(birth_day=text))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ff605",
   "metadata": {},
   "source": [
    "## Complex Module Example: Story Lab\n",
    "\n",
    "You give it a story scenario/genre and it will work through the idea:\n",
    "- first, it calls `FindInspo`, which comes up with a few catchy ideas.\n",
    "- then it drafts the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee49318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindInspo(dspy.Signature):\n",
    "    topic   = dspy.InputField()\n",
    "    blurbs  = dspy.OutputField(format=list, desc=\"3-5 catchy micro-blurbs\")\n",
    "\n",
    "class DraftPlot(dspy.Signature):\n",
    "    topic   = dspy.InputField()\n",
    "    blurbs  = dspy.InputField(format=list)\n",
    "    outline = dspy.OutputField(desc=\"bullet outline of the plot\")\n",
    "\n",
    "class BuildQuiz(dspy.Signature):\n",
    "    story      = dspy.InputField()\n",
    "    questions  = dspy.OutputField(\n",
    "        format=list,\n",
    "        desc=\"list of (Q, choices:list, correct:str) tuples\")\n",
    "\n",
    "class TeaseTweet(dspy.Signature):\n",
    "    story = dspy.InputField()\n",
    "    tweet = dspy.OutputField(desc=\"280-char teaser\")\n",
    "\n",
    "\n",
    "#  2. Custom module (3 distinct DSPy modules) \n",
    "class StoryPuzzleLab(dspy.Module):\n",
    "    \"\"\"\n",
    "    Modules used \n",
    "       ChainOfThought          inspirations & quiz\n",
    "       ChainOfThoughtWithHint  plot expansion\n",
    "       ProgramOfThought        code-driven puzzle generation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inspirer = dspy.ChainOfThought(FindInspo, n=1)\n",
    "        self.plotter  = dspy.ChainOfThoughtWithHint(DraftPlot)\n",
    "        self.quizzer  = dspy.ChainOfThought(BuildQuiz)\n",
    "        self.teaser   = dspy.ChainOfThought(TeaseTweet, max_len=70)  #  tweet\n",
    "\n",
    "    def forward(self, topic: str):\n",
    "        blurbs   = self.inspirer(topic=topic).blurbs\n",
    "        outline  = self.plotter(topic=topic, blurbs=blurbs).outline\n",
    "        quiz     = self.quizzer(story=outline).questions\n",
    "        tweet    = self.teaser(story=outline).tweet\n",
    "\n",
    "        return dict(outline=outline, quiz=quiz, tweet=tweet, inspo=blurbs)\n",
    "\n",
    "\n",
    "\n",
    "lab = StoryPuzzleLab()\n",
    "res = lab(\"Lost temple, steam-age explorers\")\n",
    "print(\"\\n INSPIRATION \\n\", res[\"inspo\"])\n",
    "print(\"\\n STORY \\n\",res[\"outline\"])\n",
    "print(\"\\n QUIZ \\n\",res['quiz'])\n",
    "print(\"\\n TWEET \\n\", res[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a7dfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf46923",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(res['quiz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeb2384",
   "metadata": {},
   "source": [
    "## So What?: Optimizers\n",
    "\n",
    "The syntax is nice and simple and whatever but what's so special about DSPy?\n",
    "\n",
    "Well notice how we haven't really defined individual *prompts*? Instead we've been defining Signatures that convey some amount of prompt information. On the backend, Signatures are parsed and converted into one of many basic boilerplate prompts.  *Optimizers* act as ways to iteratively improved those boiletplate prompts.  \n",
    "\n",
    "You define a metric associated with the Module you've constructed, you run a couple data samples through the system, then the Optimizer comes in and tweaks the prompts based on the failure modes it's picked up on.  \n",
    "\n",
    "There's a ton of different Optimizers out there that each try to modify the prompts in different ways:\n",
    "- `BootstrapFewShot` creates few-shot demonstrations\n",
    "- `MIPROv2` and `COPRO` use Bayesian search to propose better natural language instruction. \n",
    "- `BootstrapFinetune` can be used with smaller LLMs to distill the prompts into weight updates\n",
    "\n",
    "Each Optimizer accepts two arguments: \n",
    "- a `Module`\n",
    "- a metric function that returns a float\n",
    "- train/validation data: 10-300 `Examples`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f89fe",
   "metadata": {},
   "source": [
    "This example is for sentiment analysis, a pretty mundane task nowadays, but there's tons of available data for it.  For data we're using the Stanford Sentiment Treebank, a collection of single sentence rotton tomatoes reviews that are labelled as either positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce02afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n",
      "{'idx': 6, 'sentence': 'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . ', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#construct module\n",
    "sentiment = dspy.Predict(\"text -> label\")\n",
    "\n",
    "ds = load_dataset(\"sst2\", split=\"train[:20]\")\n",
    "print(\"Example\")\n",
    "print(ds[6])\n",
    "\n",
    "\n",
    "\n",
    "trainset = [\n",
    "    dspy.Example(text=ex[\"sentence\"],\n",
    "                 label=\"positive\" if ex[\"label\"] else \"negative\").with_inputs(\"text\")\n",
    "    for ex in ds\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45657b6a",
   "metadata": {},
   "source": [
    "Next we have to define our metric function, which captures some sort of validation on the task we're trying to complete. In this case it's pretty trivial: does the Module return the correct label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64e8d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def em(example, pred, *_, **__):\n",
    "    return int(example.label == pred.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e918937",
   "metadata": {},
   "source": [
    "For our first round of optimizations we're using BootstrapFewShot. This optimizer adds `k` fewshot examples to the prompt.  Essentially it automates the process of picking the examples that work the best.\n",
    "\n",
    "\n",
    "\n",
    "For tasks with small token counts you can set the number of examples pretty high. For high token count things that gets a little trickier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "743f980e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 5/20 [00:05<00:17,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "opt = BootstrapFewShot(metric=em,\n",
    "                       max_labeled_demos=4,\n",
    "                       max_bootstrapped_demos=4)\n",
    "sentiment_clf = opt.compile(sentiment, trainset=trainset)\n",
    "\n",
    "\n",
    "print(sentiment_clf(text=\"It was a delightful movie!\").label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee7082",
   "metadata": {},
   "source": [
    "Optimizers can be saved to json using `save()` and autologged to MLFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9158d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_clf.save(\"./sentiment.json\", save_program=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56363a80",
   "metadata": {},
   "source": [
    "## Different Optimizer Methods\n",
    "\n",
    "## Going Larger: Programs\n",
    "\n",
    "## Logging LLM calls for debugging\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef973678",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "### Bayesian Search\n",
    "Also called Bayesian optimization. It's a strategy for searching through a large parameter-space with only a small amount of traversals are allowed.  It tries to balance exploration with exploitation\n",
    "\n",
    "1. Start with some given prior.\n",
    "2. Try a few prompts and record their scores\n",
    "3. Fit a surrogate model: try to cheaply model the relationship between instruction -> expected score.\n",
    "4. Use the surrogate to identify potential new instruction candidates.\n",
    "5. Evaluate, update surrogate, repeat.\n",
    "\n",
    "It's basically a cheap and dirty way to get something functioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ae040",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
