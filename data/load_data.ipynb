{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"\\\n",
    "You *vertically split* the model . For `devices=4`, this would mean that the first quarter of the model layers are stored in gpu 0.\n",
    "\n",
    "During training, a forward pass is first computed in device 0, then that intermediate result is passed on to device 1, where it runs the next layers, etc etc until it finishes the forward on device 3. Then the backwards pass begins at device 3 and ripples backwards. Updates are handled all at the same time.  \n",
    "\n",
    "This splits the [[LLM Architecture#KV-Cache|KV-Cache]] and model parameters across four different devices, allowing for higher batch counts or larger models to be trained. \n",
    "\n",
    "Because the process is *sequential*, it creates *pipeline bubbles*, periods of time where the devices are not actively doing computation.  This downside can be mitigated by increasing the [[How Training Works#Microbatches|microbatch]] size, but can't be completely eliminated.\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d80654",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"\"\"\\\n",
    "\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c384ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_list=[\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c2c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
